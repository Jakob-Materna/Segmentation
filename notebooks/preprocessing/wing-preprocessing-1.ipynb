{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "df122238-e8e0-417b-a557-f05269f53357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from scipy.spatial import cKDTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fbd96c96-10a4-4929-91b3-c6f9ecb31635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# select the device for computation\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ca856d83-f420-4569-87ea-1908a29dfe09",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# segment-anything-2 functions\n",
    "def show_mask(mask, ax, random_color=False, borders = True):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask = mask.astype(np.uint8)\n",
    "    mask_image =  mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    if borders:\n",
    "        import cv2\n",
    "        contours, _ = cv2.findContours(mask,cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE) \n",
    "        # Try to smooth contours\n",
    "        contours = [cv2.approxPolyDP(contour, epsilon=0.01, closed=True) for contour in contours]\n",
    "        mask_image = cv2.drawContours(mask_image, contours, -1, (1, 1, 1, 0.5), thickness=2) \n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='.', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='.', s=marker_size, edgecolor='white', linewidth=1.25)   \n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0, 0, 0), lw=2))    \n",
    "\n",
    "def show_masks(image, masks, scores, point_coords=None, box_coords=None, input_labels=None, borders=True):\n",
    "    for i, (mask, score) in enumerate(zip(masks, scores)):\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(image)\n",
    "        show_mask(mask, plt.gca(), borders=borders)\n",
    "        if point_coords is not None:\n",
    "            assert input_labels is not None\n",
    "            show_points(point_coords, input_labels, plt.gca())\n",
    "        if box_coords is not None:\n",
    "            # boxes\n",
    "            show_box(box_coords, plt.gca())\n",
    "        if len(scores) > 1:\n",
    "            plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d4f51c00-7139-4d3e-93a3-ad2374a69bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "\n",
    "sam2_checkpoint = \"/home/wsl/bin/segment-anything-2/checkpoints/sam2_hiera_large.pt\"\n",
    "model_cfg = \"sam2_hiera_l.yaml\"\n",
    "\n",
    "sam2_model = build_sam2(model_cfg, sam2_checkpoint, device=device)\n",
    "\n",
    "predictor = SAM2ImagePredictor(sam2_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8a36c4a7-4953-4408-9a49-7f9b465e32e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_points_near_border(points, contour, border_dist_threshold):\n",
    "    filtered_points = []\n",
    "\n",
    "    # Iterate over all points\n",
    "    for point in points:\n",
    "        # Check the distance of the point to the contour\n",
    "        dist_to_contour = cv2.pointPolygonTest(contour, (point[0], point[1]), True)\n",
    "        \n",
    "        # Keep the point if it's farther from the border than the threshold\n",
    "        if dist_to_contour >= border_dist_threshold:\n",
    "            filtered_points.append(point)\n",
    "    \n",
    "    return np.array(filtered_points)\n",
    "\n",
    "    \n",
    "def find_black_area(image, window_size):\n",
    "    h, w = image.shape\n",
    "    max_density = -1\n",
    "    best_coords = (0, 0)\n",
    "\n",
    "    # Slide the window over the image\n",
    "    for y in range(0, h - window_size[1] + 1, 1):\n",
    "        for x in range(0, w - window_size[0] + 1, 1):\n",
    "            # Extract the window from the image\n",
    "            window = image[y:y + window_size[1], x:x + window_size[0]]\n",
    "\n",
    "            # Count the number of black pixels (assuming black pixels are 0)\n",
    "            black_pixel_count = np.sum(window == 0)\n",
    "\n",
    "            # Track the window with the maximum number of black pixels\n",
    "            if black_pixel_count > max_density:\n",
    "                max_density = black_pixel_count\n",
    "                best_coords = (x, y)\n",
    "\n",
    "    return best_coords, max_density\n",
    "        \n",
    "\n",
    "def remove_background(wing):\n",
    "    # Show image\n",
    "    # plt.figure(figsize=(5, 5))\n",
    "    # plt.imshow(cv2.cvtColor(wing, cv2.COLOR_BGR2RGB))\n",
    "    # plt.axis('off')\n",
    "    # plt.show()\n",
    "    \n",
    "    # expanded_image = cv2.copyMakeBorder(wing, 1000, 1000, 1000, 1000, cv2.BORDER_CONSTANT, value=[255, 255, 255])\n",
    "    expanded_image = wing\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(expanded_image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply thresholding to get a binary image\n",
    "    _, thresh = cv2.threshold(gray, 200, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Invert the binary image\n",
    "    inv_thresh = cv2.bitwise_not(thresh)\n",
    "    \n",
    "    # Show image\n",
    "    # plt.figure(figsize=(5, 5))\n",
    "    # plt.imshow(inv_thresh, cmap=\"gray\")\n",
    "    # plt.axis('off')\n",
    "    # plt.show()\n",
    "    \n",
    "    # Find contour\n",
    "    contours, _ = cv2.findContours(inv_thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    large_contours = [cnt for cnt in contours if cv2.contourArea(cnt) > 5000]\n",
    "    contour = large_contours[0]\n",
    "    \n",
    "    # Draw contours on the image for visualization\n",
    "    wing_contour_image = expanded_image.copy()\n",
    "    cv2.drawContours(wing_contour_image, large_contours, -1, (0, 0, 255), 5)\n",
    "    \n",
    "    # Show image\n",
    "    # plt.figure(figsize=(5, 5))\n",
    "    # plt.imshow(cv2.cvtColor(wing_contour_image, cv2.COLOR_BGR2RGB))\n",
    "    # plt.axis('off')\n",
    "    # plt.show()\n",
    "    \n",
    "    # Get bounding box of the contour\n",
    "    x, y, w, h = cv2.boundingRect(contour)\n",
    "    \n",
    "    # Create a dense grid of points within the bounding box\n",
    "    distance = 100  \n",
    "    height, width, channels = expanded_image.shape  \n",
    "    \n",
    "    # Create x and y coordinates\n",
    "    x_coords = np.arange(0, width, distance)\n",
    "    y_coords = np.arange(0, height, distance)\n",
    "    \n",
    "    # Create a meshgrid from the x and y coordinates\n",
    "    grid_x, grid_y = np.meshgrid(x_coords, y_coords)\n",
    "    \n",
    "    # Stack the x and y coordinates into a single array of points\n",
    "    grid_points = np.vstack([grid_x.ravel(), grid_y.ravel()]).T\n",
    "    \n",
    "    # Convert the NumPy array to a list of tuples with standard integers\n",
    "    grid_points = [(int(x), int(y)) for x, y in grid_points]\n",
    "    \n",
    "    inside_points = []\n",
    "    \n",
    "    # Check if points are inside the contour\n",
    "    for point in grid_points:\n",
    "        if cv2.pointPolygonTest(contour, (point[0], point[1]), False) >= 0:\n",
    "            inside_points.append(point)\n",
    "    \n",
    "    # inside_points = np.array(inside_points)\n",
    "    \n",
    "    filtered_points = remove_points_near_border(inside_points, contour, 25)\n",
    "    # Find the coordinates of the area with the highest density of black pixels\n",
    "    # best_coords, max_density = find_black_area(gray, window_size)\n",
    "\n",
    "    \n",
    "    window_size = (5, 5)\n",
    "    best_coords, max_density = find_black_area(gray, window_size)\n",
    "    \n",
    "    # Plot the contour and the selected points\n",
    "    # Plot the contour and the selected points\n",
    "    # plt.figure(figsize=(5, 5))\n",
    "    # plt.imshow(cv2.cvtColor(wing_contour_image, cv2.COLOR_BGR2RGB))\n",
    "    # plt.scatter(best_coords[0], best_coords[1], c=\"green\", s=10)\n",
    "    # plt.scatter(filtered_points[:, 0], filtered_points[:, 1], c=\"red\", s=5)\n",
    "    # plt.axis('off')\n",
    "    # plt.show()\n",
    "\n",
    "    \n",
    "    filtered_points = np.vstack([filtered_points, best_coords])\n",
    "\n",
    "    # Sam background removal\n",
    "    # Convert the points list to a numpy array\n",
    "    image_points = np.array(filtered_points)\n",
    "    image_labels = np.array([1] * len(filtered_points))\n",
    "    \n",
    "    predictor.set_image(expanded_image)\n",
    "    \n",
    "    mask, score, _ = predictor.predict(\n",
    "        point_coords=image_points,\n",
    "        point_labels=image_labels,\n",
    "        multimask_output=False,\n",
    "    )\n",
    "    sorted_ind = np.argsort(score)[::-1]\n",
    "    mask = mask[sorted_ind]\n",
    "    score = score[sorted_ind]\n",
    "    \n",
    "    # show_masks(expanded_image, masks, scores, point_coords=point, input_labels=label, borders=True)\n",
    "    \n",
    "    # Remove extra dimension\n",
    "    mask = mask.squeeze()\n",
    "    \n",
    "    # Create a white image of the same size as the original image\n",
    "    white_image = np.ones_like(expanded_image) * 255\n",
    "    \n",
    "    # Apply the mask to each channel (no extra dimension added)\n",
    "    wing_image = np.where(mask[:, :, None], expanded_image, white_image)\n",
    "    \"\"\"\n",
    "    # Show image\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.imshow(cv2.cvtColor(wing_image, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \"\"\"\n",
    "    return wing_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "272db006-808d-4221-a513-eca4923dc8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_wing(wing_image):\n",
    "    expanded_image = cv2.copyMakeBorder(wing_image, 1000, 1000, 1000, 1000, cv2.BORDER_CONSTANT, value=[255, 255, 255])\n",
    "    \n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(expanded_image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply thresholding to get a binary image\n",
    "    _, thresh = cv2.threshold(gray, 200, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Invert the binary image\n",
    "    inv_thresh = cv2.bitwise_not(thresh)\n",
    "    \"\"\"\n",
    "    # Show image\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(inv_thresh, cmap=\"gray\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \"\"\"\n",
    "    # Find contour\n",
    "    contours, _ = cv2.findContours(inv_thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    large_contours = [cnt for cnt in contours if cv2.contourArea(cnt) > 5000]\n",
    "    contour = large_contours[0]\n",
    "    \n",
    "    # Draw contours on the image for visualization\n",
    "    wing_contour_image = expanded_image.copy()\n",
    "    cv2.drawContours(wing_contour_image, large_contours, -1, (0, 0, 255), 5)\n",
    "    \"\"\"\n",
    "    # Show image\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(wing_contour_image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \"\"\"\n",
    "    # Get the minimum area rectangle\n",
    "    rect = cv2.minAreaRect(contour)\n",
    "    \n",
    "    # Get the four points of the rectangle\n",
    "    box = cv2.boxPoints(rect)\n",
    "    \n",
    "    # Convert the points to integers\n",
    "    box = np.int0(box)\n",
    "    \n",
    "    # Draw the rotated rectangle\n",
    "    contour_image = expanded_image.copy()\n",
    "    cv2.drawContours(contour_image, [box], 0, (0, 0, 255), 5)\n",
    "    \n",
    "    # Get the rectangle's center, size (width, height), and angle\n",
    "    box_center, box_size, angle = rect\n",
    "    \n",
    "    # Ensure width is the longest side (width > height)\n",
    "    width, height = box_size\n",
    "    if height > width:\n",
    "        width, height = height, width\n",
    "        angle -= 90  # Rotate to make the longest side horizontal\n",
    "    \n",
    "    # Get the rotation matrix to rotate the image around the rectangle's center\n",
    "    rotation_matrix = cv2.getRotationMatrix2D(box_center, angle, 1.0)\n",
    "    \n",
    "    # Rotate the entire image\n",
    "    rotated_image = cv2.warpAffine(expanded_image, rotation_matrix, (expanded_image.shape[1], expanded_image.shape[0]))\n",
    "    \n",
    "    # Convert the center and size to integers\n",
    "    box_center = (int(box_center[0]), int(box_center[1]))\n",
    "    width, height = int(width), int(height)\n",
    "    \n",
    "    # Crop the aligned rectangle from the rotated image\n",
    "    cropped_image = cv2.getRectSubPix(rotated_image, (width+20, height+20), box_center)\n",
    "    \"\"\"\n",
    "    # Show image\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(contour_image)\n",
    "    plt.axis('on')\n",
    "    plt.show()\n",
    "\n",
    "    # Show image\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(cropped_image)\n",
    "    plt.axis('on')\n",
    "    plt.show()\n",
    "    \"\"\"\n",
    "    return(cropped_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "18d79300-1012-44c4-b574-5ab54930a1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing directory: Hive01_Sheet_01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_70125/1809599758.py:41: DeprecationWarning: `np.int0` is a deprecated alias for `np.intp`.  (Deprecated NumPy 1.24)\n",
      "  box = np.int0(box)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m wing \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(input_file)\n\u001b[1;32m     32\u001b[0m wing \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(wing\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m---> 33\u001b[0m wing \u001b[38;5;241m=\u001b[39m \u001b[43mremove_background\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwing\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m cropped_image \u001b[38;5;241m=\u001b[39m crop_wing(wing)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# cropped_image = cv2.cvtColor(cropped_image, cv2.COLOR_BGR2RGB)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[45], line 133\u001b[0m, in \u001b[0;36mremove_background\u001b[0;34m(wing)\u001b[0m\n\u001b[1;32m    129\u001b[0m image_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(filtered_points))\n\u001b[1;32m    131\u001b[0m predictor\u001b[38;5;241m.\u001b[39mset_image(expanded_image)\n\u001b[0;32m--> 133\u001b[0m mask, score, _ \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpoint_coords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_points\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpoint_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmultimask_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m sorted_ind \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(score)[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    139\u001b[0m mask \u001b[38;5;241m=\u001b[39m mask[sorted_ind]\n",
      "File \u001b[0;32m~/bin/segment-anything-2/sam2/sam2_image_predictor.py:287\u001b[0m, in \u001b[0;36mSAM2ImagePredictor.predict\u001b[0;34m(self, point_coords, point_labels, box, mask_input, multimask_output, return_logits, normalize_coords)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn image must be set with .set_image(...) before mask prediction.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m     )\n\u001b[1;32m    285\u001b[0m \u001b[38;5;66;03m# Transform input prompts\u001b[39;00m\n\u001b[0;32m--> 287\u001b[0m mask_input, unnorm_coords, labels, unnorm_box \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prep_prompts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpoint_coords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoint_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize_coords\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m masks, iou_predictions, low_res_masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict(\n\u001b[1;32m    292\u001b[0m     unnorm_coords,\n\u001b[1;32m    293\u001b[0m     labels,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    297\u001b[0m     return_logits\u001b[38;5;241m=\u001b[39mreturn_logits,\n\u001b[1;32m    298\u001b[0m )\n\u001b[1;32m    300\u001b[0m masks_np \u001b[38;5;241m=\u001b[39m masks\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/bin/segment-anything-2/sam2/sam2_image_predictor.py:314\u001b[0m, in \u001b[0;36mSAM2ImagePredictor._prep_prompts\u001b[0;34m(self, point_coords, point_labels, box, mask_logits, normalize_coords, img_idx)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m point_coords \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    312\u001b[0m         point_labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    313\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpoint_labels must be supplied if point_coords is supplied.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 314\u001b[0m     point_coords \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpoint_coords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     unnorm_coords \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transforms\u001b[38;5;241m.\u001b[39mtransform_coords(\n\u001b[1;32m    318\u001b[0m         point_coords, normalize\u001b[38;5;241m=\u001b[39mnormalize_coords, orig_hw\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_orig_hw[img_idx]\n\u001b[1;32m    319\u001b[0m     )\n\u001b[1;32m    320\u001b[0m     labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(point_labels, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_dir = \"/mnt/c/Projects/Master/Data/WingScanCrops/\"\n",
    "output_dir = \"/mnt/c/Projects/Master/Data/Segmented/\"\n",
    "\n",
    "# Ensure the input directory exists\n",
    "if not os.path.exists(input_dir):\n",
    "    raise FileNotFoundError(f\"Input directory '{input_dir}' was not found.\")\n",
    "\n",
    "# Create the output directory\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# List all directories in the specified directory\n",
    "all_directories = [entry for entry in os.listdir(input_dir)]\n",
    "\n",
    "for dirname in all_directories:\n",
    "    if not \"Hive\" in dirname:\n",
    "        print(f\"Skipping directory: {dirname}\")\n",
    "        continue\n",
    "        \n",
    "    print(f\"Processing directory: {dirname}\")\n",
    "    input_subdir = input_dir + \"/\" + dirname + \"/\"\n",
    "    output_subdir = output_dir + \"/\" + dirname + \"/\"\n",
    "    \n",
    "    # Create the output directory\n",
    "    os.makedirs(output_subdir, exist_ok=True)\n",
    "\n",
    "    # Find jpg files\n",
    "    jpg_files = [file for file in os.listdir(input_subdir) if file.endswith('.jpg')]\n",
    "    for jpg_file in jpg_files:\n",
    "        input_file = input_subdir + jpg_file\n",
    "        output_file = output_subdir + jpg_file\n",
    "        wing = Image.open(input_file)\n",
    "        wing = np.array(wing.convert(\"RGB\"))\n",
    "        wing = remove_background(wing)\n",
    "        cropped_image = crop_wing(wing)\n",
    "        \n",
    "        # cropped_image = cv2.cvtColor(cropped_image, cv2.COLOR_BGR2RGB)\n",
    "        cropped_image = Image.fromarray(cropped_image)\n",
    "        if \"Left\" in jpg_file:\n",
    "            cropped_image = cropped_image.transpose(method=Image.FLIP_LEFT_RIGHT)\n",
    "            # cropped_image = cv2.cvtColor(cropped_image, cv2.COLOR_BGR2RGB)\n",
    "            # cropped_image = Image.fromarray(cropped_image)\n",
    "    \n",
    "        # Show image\n",
    "        # plt.figure(figsize=(5, 5))\n",
    "        # plt.imshow(cropped_image)\n",
    "        # plt.axis('on')\n",
    "        # plt.show()\n",
    "        \n",
    "        cropped_image.save(output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c317f4c-d995-44a8-be6b-77914d381201",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e673e8ed-551f-44e3-b828-78e3bae8d648",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
